# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DEEP RESEARCH - CONFIGURAÇÃO
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#
# Copie este arquivo para .env e preencha com suas chaves de API.
# Este arquivo NÃO deve ser commitado no git.
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ──────────────────────────────────────────────────────────────────────────────
# CHAVES DE API (OBRIGATÓRIAS)
# ──────────────────────────────────────────────────────────────────────────────

# Chave da API OpenAI (obrigatória para LLM)
# Obtenha em: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx

# Chave da API Jina AI (obrigatória para busca e leitura)
# Obtenha em: https://jina.ai/
JINA_API_KEY=jina_xxxxxxxxxxxxx

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DO LLM (Modelo de Linguagem)
# ──────────────────────────────────────────────────────────────────────────────

# Provider de LLM: openai, anthropic, local (padrão: openai)
LLM_PROVIDER=openai

# Modelo principal para geração de texto
# Exemplos OpenAI: gpt-4.1-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Exemplos Anthropic: claude-3-opus, claude-3-sonnet
# Padrão: gpt-4.1-mini
LLM_MODEL=gpt-4.1-mini

# URL base customizada da API (opcional, para providers locais ou proxies)
# Exemplos:
#   - Ollama: http://localhost:11434/api
#   - OpenAI proxy: https://proxy.example.com/v1
# LLM_API_BASE_URL=http://localhost:11434/api

# Temperatura padrão para geração (0.0 a 2.0)
# Menor = mais determinístico, Maior = mais criativo
# Padrão: 0.7
LLM_TEMPERATURE=0.7

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DE EMBEDDINGS
# ──────────────────────────────────────────────────────────────────────────────
#
# IMPORTANTE: Você pode usar um provider de LLM (ex: OpenAI) para texto
# e outro provider (ex: Jina) para embeddings!
#

# Provider de embeddings: openai, jina (padrão: openai)
# Use "jina" para usar Jina Embeddings junto com LLM da OpenAI
EMBEDDING_PROVIDER=jina

# Modelo OpenAI para embeddings (usado quando EMBEDDING_PROVIDER=openai)
# Opções: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Padrão: text-embedding-3-small
LLM_EMBEDDING_MODEL=text-embedding-3-small

# Modelo Jina para embeddings (usado quando EMBEDDING_PROVIDER=jina)
# Opções: jina-embeddings-v4, jina-embeddings-v3, jina-embeddings-v2-base-en
# Padrão: jina-embeddings-v4
JINA_EMBEDDING_MODEL=jina-embeddings-v4

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DO AGENTE
# ──────────────────────────────────────────────────────────────────────────────

# Número mínimo de steps antes de permitir resposta (ANSWER)
# 0 = permite resposta imediata, 1 = força pelo menos uma pesquisa
# Padrão: 1 (recomendado)
AGENT_MIN_STEPS=1

# Permite resposta direta sem pesquisa para perguntas triviais
# true = permite, false = sempre pesquisa primeiro
# Padrão: false (recomendado para qualidade)
AGENT_ALLOW_DIRECT_ANSWER=false

# Budget máximo de tokens por pesquisa
# Padrão: 1000000 (1 milhão)
AGENT_TOKEN_BUDGET=1000000

# Máximo de URLs para ler por step
# Padrão: 10
AGENT_MAX_URLS_PER_STEP=10

# Máximo de queries de busca por step
# Padrão: 5
AGENT_MAX_QUERIES_PER_STEP=5

# Máximo de falhas consecutivas antes de forçar resposta
# Padrão: 3
AGENT_MAX_FAILURES=3

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DO RUNTIME TOKIO
# ──────────────────────────────────────────────────────────────────────────────

# Número fixo de threads do runtime (opcional)
# Se não definido, usa cálculo dinâmico: min(cpu_cores, TOKIO_MAX_THREADS)
# TOKIO_THREADS=4

# Máximo de threads para cálculo dinâmico
# Padrão: 16
TOKIO_MAX_THREADS=16

# Máximo de blocking threads
# Padrão: 512
TOKIO_MAX_BLOCKING=512

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DO WEBREADER
# ──────────────────────────────────────────────────────────────────────────────

# Preferência de método para leitura de URLs
# Opções:
#   - jina: Apenas Jina Reader API (mais confiável, depende de API externa)
#   - rust: Apenas Rust local (mais rápido, pode falhar em sites complexos)
#   - compare: Rust primeiro, Jina como fallback (recomendado)
# Padrão: compare
WEBREADER=compare

# ──────────────────────────────────────────────────────────────────────────────
# CONFIGURAÇÃO DE IDIOMA
# ──────────────────────────────────────────────────────────────────────────────

# Idioma das respostas geradas
# Opções: pt-br, en, es, fr, de, it, ja, zh, ko
# Padrão: pt-br
RESPONSE_LANGUAGE=pt-br
