- id: setup-person-2
  content: Configurar ambiente local para Pessoa 2 (Personas/Busca) e rodar benchmarks de personas
  status: pending
- id: setup-person-3
  content: Configurar ambiente local para Pessoa 3 (Performance) e rodar bench E2E para baseline
  status: pending

---

# Plano de Testes Distribu√≠dos: DeepResearch AI (Rust)

Objetivo - micro passos
Quais testes v√£o ser necess√°rios
Evid√™ncias - estabalecer as evid√™ncias necess√°rias
compara√ß√£o de benchmark com antes e depois

## üë• Divis√£o de Responsabilidades

### üë§ Pessoa 2: Personas, Busca e Avalia√ß√£o (Dom√≠nio)

**Foco:** Garantir que as diferentes personalidades (Research, Academic, etc.) se comportem como esperado, que a busca retorne resultados relevantes e que o sistema de avalia√ß√£o julgue corretamente as respostas.

# Plano: Sistema de Personas, Busca e Avaliacao (Pessoa 2)

## Objetivo Principal

Criar um sistema de raciocinio passo a passo que seja:

1. **Agn√≥stico de LLM** - Funciona com qualquer modelo de linguagem
2. **Escal√°vel** - Permite adicionar novas personas seguindo um padr√£o
3. **Verific√°vel** - Gera evid√™ncias de funcionamento em cada etapa
4. **Compar√°vel** - Pode ser medido contra a implementa√ß√£o TypeScript existente

---

## Fase 1: Micro-Passos para Personas

### 1.1 Padronizar o Trait `CognitivePersona`

**Arquivo:** `rust-implementation/src/personas/traits.rs`

**Por que:** O trait atual define a interface mas falta:

- Sistema de logging estruturado
- Coleta de m√©tricas por persona
- Contexto de execu√ß√£o rastre√°vel

**Passos:**

1. Adicionar campo `execution_id: Uuid` no `QueryContext` para rastrear cada execu√ß√£o
2. Criar struct `PersonaExecutionMetrics` com:

   - `persona_name: String`
   - `start_time: Instant`
   - `end_time: Instant`
   - `input_tokens: usize`
   - `output_query: String`
   - `memory_allocated: usize`

3. Modificar `expand_query` para retornar `(SerpQuery, PersonaExecutionMetrics)`

### 1.2 Implementar Sistema de Registro de Personas

**Por que:** Para adicionar novas personas dinamicamente seguindo um padr√£o.

**Passos:**

1. Criar `PersonaRegistry` em `src/personas/mod.rs`:

   ```rust
   pub struct PersonaRegistry {
       personas: HashMap<String, PersonaBox>,
       schemas: HashMap<String, PersonaSchema>,
   }
   ```

2. Implementar m√©todos `register()`, `unregister()`, `list_available()`
3. Criar arquivo JSON de configura√ß√£o para definir personas sem recompilar

### 1.3 Criar Sistema de Valida√ß√£o de Persona

**Por que:** Garantir que novas personas seguem o contrato esperado.

**Passos:**

1. Criar trait `PersonaValidator` com m√©todos:

   - `validate_name()` - N√£o vazio, √∫nico
   - `validate_focus()` - Descri√ß√£o m√≠nima de 10 caracteres
   - `validate_weight()` - Entre 0.0 e 2.0
   - `validate_output()` - Query n√£o vazia, formato v√°lido

2. Implementar testes autom√°ticos de conformidade

---

## Fase 2: Micro-Passos para Busca

### 2.1 Implementar Rastreamento de Fluxo de Dados

**Arquivo:** `rust-implementation/src/search.rs`

**Por que:** Necess√°rio saber onde cada dado foi acionado e para onde foi.

**Passos:**

1. Criar `SearchTrace`:

   ```rust
   pub struct SearchTrace {
       pub trace_id: Uuid,
       pub query_origin: QueryOrigin, // Persona, User, Reflection
       pub query_sent: SerpQuery,
       pub api_called: String,
       pub request_timestamp: DateTime<Utc>,
       pub response_timestamp: DateTime<Utc>,
       pub results_count: usize,
       pub bytes_received: usize,
       pub urls_extracted: Vec<String>,
   }
   ```

2. Modificar `SearchClient::search` para retornar `(SearchResult, SearchTrace)`
3. Implementar `SearchTraceCollector` que agrega todas as traces de uma execu√ß√£o

### 2.2 Implementar M√©tricas de Busca

**Por que:** Comparar performance com TypeScript.

**Passos:**

1. Criar `SearchMetrics`:

   - `latency_p50`, `latency_p95`, `latency_p99`
   - `success_rate`
   - `avg_results_per_query`
   - `cache_hit_rate`
   - `bytes_per_second`

2. Implementar coleta via middleware no `SearchClient`

### 2.3 Implementar Cache de Resultados

**Por que:** Reduzir chamadas repetidas e medir efici√™ncia.

**Passos:**

1. Criar `SearchCache` com TTL configur√°vel
2. Implementar serializa√ß√£o/deserializa√ß√£o de resultados
3. Adicionar m√©tricas de cache hit/miss

---

## Fase 3: Micro-Passos para Avalia√ß√£o

### 3.1 Implementar Pipeline de Avalia√ß√£o Observ√°vel

**Arquivo:** `rust-implementation/src/evaluation/pipeline.rs`

**Por que:** Necess√°rio saber como cada avalia√ß√£o funcionou e quanto tempo durou.

**Passos:**

1. Criar `EvaluationTrace`:

   ```rust
   pub struct EvaluationTrace {
       pub trace_id: Uuid,
       pub eval_type: EvaluationType,
       pub question: String,
       pub answer_hash: String, // N√£o logar resposta inteira
       pub start_time: Instant,
       pub end_time: Instant,
       pub llm_tokens_used: u32,
       pub passed: bool,
       pub confidence: f32,
       pub reasoning_length: usize,
   }
   ```

2. Modificar `EvaluationPipeline::evaluate_single` para coletar traces
3. Criar `EvaluationReport` agregando todas as avalia√ß√µes

### 3.2 Implementar Determina√ß√£o de Tipos de Avalia√ß√£o

**Por que:** O TypeScript tem `evaluateQuestion()` que determina quais avalia√ß√µes aplicar.

**Passos:**

1. Portar l√≥gica de `src/tools/evaluator.ts` linhas 560-596
2. Implementar `determine_required_evaluations()` sem depender de LLM:

   - Regras baseadas em keywords
   - Fallback para LLM se necess√°rio

3. Criar testes com os mesmos exemplos do TypeScript

### 3.3 Implementar Prompts de Avalia√ß√£o

**Por que:** Cada tipo de avalia√ß√£o precisa de prompts espec√≠ficos.

**Passos:**

1. Criar m√≥dulo `src/evaluation/prompts.rs`
2. Portar prompts de `evaluator.ts`:

   - `getDefinitivePrompt` -> linhas 49-154
   - `getFreshnessPrompt` -> linhas 156-219
   - `getCompletenessPrompt` -> linhas 221-310
   - `getPluralityPrompt` -> linhas 312-357
   - `getRejectAllAnswersPrompt` -> linhas 11-46

3. Parametrizar prompts para suportar m√∫ltiplos idiomas

---

## Testes Necess√°rios

### Testes Unit√°rios de Personas

| Teste | Descri√ß√£o | Evid√™ncia Esperada |

|-------|-----------|-------------------|

| `test_persona_creation` | Cada persona pode ser instanciada | Todas 7 personas criadas sem panic |

| `test_persona_expand_query` | Cada persona expande query corretamente | Output n√£o vazio, cont√©m termos esperados |

| `test_persona_weight_range` | Peso dentro de 0.0-2.0 | Assert `weight >= 0.0 && weight <= 2.0` |

| `test_persona_thread_safety` | Personas s√£o Send+Sync | Compila√ß√£o com `par_iter()` funciona |

| `test_persona_uniqueness` | Queries expandidas s√£o √∫nicas entre personas | HashSet de queries tem 7 elementos |

| `test_persona_determinism` | Mesma entrada = mesma sa√≠da (exceto random) | 2 execu√ß√µes com seed fixo s√£o iguais |

### Testes Unit√°rios de Busca

| Teste | Descri√ß√£o | Evid√™ncia Esperada |

|-------|-----------|-------------------|

| `test_search_mock` | Mock retorna resultados esperados | Snippets e URLs presentes |

| `test_hostname_extraction` | Extra√ß√£o de hostname funciona | `"https://example.com/path" -> "example.com"` |

| `test_hostname_boost` | Sites confi√°veis recebem boost | Wikipedia > 1.0, random = 1.0 |

| `test_path_boost` | Paths de docs recebem boost | `/docs/` > 1.0, `/about` = 1.0 |

| `test_score_calculation` | Score final calculado corretamente | `final = weight * freq * hostname * path * rerank` |

| `test_search_batch` | Busca em batch funciona | N queries -> N resultados |

### Testes Unit√°rios de Avalia√ß√£o

| Teste | Descri√ß√£o | Evid√™ncia Esperada |

|-------|-----------|-------------------|

| `test_eval_type_config` | Cada tipo tem config padr√£o | Todos 5 tipos retornam `EvaluationConfig` |

| `test_freshness_threshold` | Thresholds por t√≥pico corretos | Finance=0.1d, News=1d, Tech=7d, etc |

| `test_eval_result_success` | Resultado de sucesso criado | `passed=true`, `confidence>0` |

| `test_eval_result_failure` | Resultado de falha criado | `passed=false`, `suggestions.len()>0` |

| `test_pipeline_early_fail` | Pipeline para na primeira falha | 2 avalia√ß√µes executadas, 3 ignoradas |

| `test_determine_eval_types` | Determina tipos corretos | "What is X?" -> `[Definitive]` |

### Testes de Integra√ß√£o

| Teste | Descri√ß√£o | Evid√™ncia Esperada |

|-------|-----------|-------------------|

| `test_persona_to_search` | Personas geram queries que buscam resultados | 7 queries -> pelo menos 1 resultado cada |

| `test_search_to_eval` | Resultados de busca podem ser avaliados | SearchResult -> EvaluationContext v√°lido |

| `test_full_pipeline` | Fluxo completo funciona | Question -> Answer com evid√™ncias |

---

## Evid√™ncias a Coletar

### 1. Evid√™ncias de Personas Funcionando

```rust
pub struct PersonaEvidenceReport {
    pub execution_id: Uuid,
    pub timestamp: DateTime<Utc>,
    pub original_query: String,
    pub personas_executed: Vec<PersonaEvidence>,
    pub total_queries_generated: usize,
    pub unique_queries: usize,
    pub total_execution_time: Duration,
    pub memory_peak: usize,
}

pub struct PersonaEvidence {
    pub persona_name: &'static str,
    pub focus: &'static str,
    pub weight: f32,
    pub input_received: String,
    pub output_generated: SerpQuery,
    pub execution_time: Duration,
    pub was_applicable: bool,
}
```

### 2. Evid√™ncias de Busca Funcionando

```rust
pub struct SearchEvidenceReport {
    pub execution_id: Uuid,
    pub queries_sent: Vec<SearchQueryEvidence>,
    pub total_api_calls: usize,
    pub total_bytes_transferred: usize,
    pub total_urls_discovered: usize,
    pub unique_hostnames: HashSet<String>,
    pub latency_stats: LatencyStats,
}

pub struct SearchQueryEvidence {
    pub query: SerpQuery,
    pub source_persona: Option<String>,
    pub api_endpoint: String,
    pub request_time: DateTime<Utc>,
    pub response_time: DateTime<Utc>,
    pub http_status: u16,
    pub results_count: usize,
    pub urls_extracted: Vec<UrlEvidence>,
}

pub struct UrlEvidence {
    pub url: String,
    pub hostname: String,
    pub hostname_boost: f32,
    pub path_boost: f32,
    pub final_score: f32,
}
```

### 3. Evid√™ncias de Avalia√ß√£o Funcionando

```rust
pub struct EvaluationEvidenceReport {
    pub execution_id: Uuid,
    pub question: String,
    pub answer_length: usize,
    pub evaluations_required: Vec<EvaluationType>,
    pub evaluations_executed: Vec<EvaluationEvidence>,
    pub final_verdict: bool,
    pub total_evaluation_time: Duration,
    pub total_llm_tokens: u32,
}

pub struct EvaluationEvidence {
    pub eval_type: EvaluationType,
    pub prompt_generated: bool, // N√£o logar prompt inteiro
    pub prompt_length: usize,
    pub llm_called: bool,
    pub llm_latency: Duration,
    pub llm_tokens_used: u32,
    pub result_passed: bool,
    pub result_confidence: f32,
    pub reasoning_length: usize,
    pub suggestions_count: usize,
}
```

---

## Compara√ß√£o com TypeScript

### M√©tricas a Comparar

| M√©trica | Como Medir TS | Como Medir Rust | Benchmark |

|---------|---------------|-----------------|-----------|

| Tempo de expans√£o de queries | `console.time()` em `agent.ts` | `Instant::now()` + criterion | `cargo bench --bench personas_bench` |

| Lat√™ncia de busca | `Date.now()` em `jina-search.ts` | `SearchTrace.response_time - request_time` | `cargo bench --bench search_bench` |

| Tempo de avalia√ß√£o | Logs em `evaluator.ts` | `EvaluationTrace.end_time - start_time` | `cargo bench --bench evaluation_bench` |

| Uso de mem√≥ria | `process.memoryUsage()` | `std::alloc::System` + metrics | `/proc/self/status` em Linux |

| Tokens LLM | `TokenTracker` | `TokenTracker` (j√° existe) | Comparar totais por execu√ß√£o |

| Throughput | Requests/segundo em produ√ß√£o | `Throughput::Elements` no criterion | Benchmark E2E |

### Formato do Relat√≥rio de Compara√ß√£o

```markdown
# Relat√≥rio de Compara√ß√£o: Rust vs TypeScript

## Resumo Executivo

- Rust X% mais r√°pido em expans√£o de queries
- Rust Y% menos mem√≥ria em pico
- Rust Z% menos tokens LLM (otimiza√ß√£o de prompts)

## Personas

| Persona        | TS (ms) | Rust (ms) | Diferen√ßa |
| -------------- | ------- | --------- | --------- |
| Expert Skeptic | 12.3    | 0.8       | -93%      |
| Detail Analyst | 11.1    | 0.7       | -94%      |
| ...            | ...     | ...       | ...       |

## Busca

| Opera√ß√£o            | TS (ms) | Rust (ms) | Diferen√ßa |
| ------------------- | ------- | --------- | --------- |
| Hostname extraction | 0.5     | 0.001     | -99%      |
| Score calculation   | 2.1     | 0.003     | -99%      |
| ...                 | ...     | ...       | ...       |

## Avalia√ß√£o

| Tipo       | TS Tokens | Rust Tokens | Diferen√ßa |
| ---------- | --------- | ----------- | --------- |
| Definitive | 450       | 420         | -7%       |
| Freshness  | 380       | 350         | -8%       |
| ...        | ...       | ...         | ...       |
```

---

## Comandos de Execu√ß√£o

```bash
# Testes unit√°rios isolados
cargo test personas:: --release
cargo test search:: --release
cargo test evaluation:: --release

# Benchmarks
cargo bench --bench personas_bench -- --save-baseline rust_v1
cargo bench --bench search_bench -- --save-baseline rust_v1
cargo bench --bench evaluation_bench -- --save-baseline rust_v1

# Gerar relat√≥rio de evid√™ncias
cargo run --release --bin evidence_report -- --output evidence_report.json

# Comparar com TypeScript
npm run benchmark # No diret√≥rio TS
cargo run --release --bin compare_ts_rust -- --ts-results ts_bench.json --rust-results rust_bench.json
```

---

## Checklist de Conclus√£o

- [ ] Todas as 7 personas passam nos testes unit√°rios
- [ ] Orquestrador executa em paralelo sem deadlock
- [ ] Busca retorna resultados e coleta m√©tricas
- [ ] Avalia√ß√£o determina tipos corretamente
- [ ] Pipeline de avalia√ß√£o funciona com early-fail
- [ ] Evid√™ncias s√£o coletadas em formato estruturado
- [ ] Benchmarks executam sem erros
- [ ] Relat√≥rio de compara√ß√£o gerado
- [ ] Performance Rust >= TypeScript em todas m√©tricas

### üë§ Pessoa 3: Performance, SIMD e End-to-End (Sistema)

**Foco:** Garantir que o sistema seja r√°pido (otimiza√ß√µes de baixo n√≠vel/SIMD), que a CLI funcione e que o fluxo completo (E2E) n√£o quebre sob carga.

- **Arquivos Principais:**
  - `src/performance/*` (simd.rs)
  - `src/main.rs` (CLI e entrypoint)
  - `src/lib.rs` (Integra√ß√£o geral)
- **Comandos de Teste (Isolados):**
  - Testes de Performance: `cargo test performance::`
  - Benchmark SIMD: `cargo bench --bench simd_bench`
  - Benchmark E2E (Fluxo Completo): `cargo bench --bench e2e_bench`
  - Verifica√ß√£o de Build Final: `cargo build --release`

## üöÄ Fluxo de Trabalho Sugerido

1. Cada pessoa deve criar uma branch separada (ex: `test/agent-fix`, `test/persona-update`).
2. Utilizar os comandos de teste isolados listados acima para n√£o esperar a su√≠te inteira rodar.
3. Reportar falhas categorizadas por √°rea (Agente vs. Dom√≠nio vs. Performance).
